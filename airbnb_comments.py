# -*- coding: utf-8 -*-
"""Airbnb comments

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tYmW1lfBWNeEgp3w77wgLTkwNQWSEUax
"""

import pandas as pd
import numpy as np
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt

# Download NLTK resources (if not already downloaded)
nltk.download('vader_lexicon')

# Load the Excel file
file_path = "/content/Airbnb data(rev).xlsx"
df = pd.read_excel(file_path)

# Extract the "comments" column
comments = df["comments"]

# Initialize the Sentiment Intensity Analyzer
sid = SentimentIntensityAnalyzer()

# Perform sentiment analysis and classify comments
sentiments = []
for comment in comments:
    if isinstance(comment, str):  # Check if comment is a string
        sentiment_score = sid.polarity_scores(comment)['compound']
        if sentiment_score >= 0.05:
            sentiments.append("Positive")
        elif sentiment_score <= -0.05:
            sentiments.append("Negative")
        else:
            sentiments.append("Neutral")
    else:
        sentiments.append("Neutral")  # Treat non-string comments as neutral

# Add the sentiment column to the DataFrame
df["sentiment"] = sentiments

# Plot the sentiment distribution
sentiment_counts = df["sentiment"].value_counts()
sentiment_counts.plot(kind="bar", color=["green", "red", "blue"])
plt.title("Sentiment Distribution of Airbnb Comments")
plt.xlabel("Sentiment")
plt.ylabel("Frequency")
plt.show()

# Convert comments to strings
comments = [str(comment) for comment in comments]

# Tokenize the comments
tokens = ' '.join(comments).split()

# Calculate word frequencies
word_freq = Counter(tokens)

# Plot the most common words
plt.figure(figsize=(12, 6))
word_freq.most_common(20)

# Convert comments to strings
comments = [str(comment) for comment in comments]

# Select the first 1,000 comments
comments_subset = comments[:1000]

# Tokenize the comments
tokenized_comments = [comment.split() for comment in comments_subset]

# Create a dictionary and corpus
dictionary = corpora.Dictionary(tokenized_comments)
corpus = [dictionary.doc2bow(text) for text in tokenized_comments]

# Perform LDA
lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)

# Print topics
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic: {idx} \nWords: {topic}")

# Add the sentiment column to the DataFrame
df["sentiment"] = sentiments

# Group by date and sentiment, calculate counts
sentiment_counts = df.groupby(['date', 'sentiment']).size().unstack(fill_value=0)

# Calculate percentage of each sentiment
sentiment_counts_percentage = sentiment_counts.div(sentiment_counts.sum(axis=1), axis=0) * 100

# Plot the sentiment trends over time
plt.figure(figsize=(12, 6))
sentiment_counts_percentage.plot(kind='area', stacked=True)
plt.title('Sentiment Trends Over Time')
plt.xlabel('Date')
plt.ylabel('Percentage (%)')
plt.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

pip install sumy

import nltk
nltk.download('punkt')